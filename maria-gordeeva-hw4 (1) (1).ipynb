{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":92556,"databundleVersionId":11029200,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport catboost\nimport lightgbm as lgb\nimport gc\nfrom sklearn.neighbors import NearestNeighbors","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:56:03.120749Z","iopub.execute_input":"2025-02-13T16:56:03.121081Z","iopub.status.idle":"2025-02-13T16:56:03.126470Z","shell.execute_reply.started":"2025-02-13T16:56:03.121054Z","shell.execute_reply":"2025-02-13T16:56:03.125509Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"If you are wondering which 3 libraries you can use, here is an example:\n\nFor training models, you can, for example, use gradient boosting from the libraries: \n1) Lightgbm\n2) Catboost\n3) Sklearn\n4) Pyboost\n5) Xgboost\n\nAlso you can use for a fitting neural network:\n1) Torch\n2) Tensorflow\n\nOnce you have made predictions using the 3 models for the test, you can average them or aggregate them in some other way.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/ioai-2025-preparation-class-lesson-4-homework/train.csv')\nsub = pd.read_csv('/kaggle/input/ioai-2025-preparation-class-lesson-4-homework/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:56:04.947047Z","iopub.execute_input":"2025-02-13T16:56:04.947388Z","iopub.status.idle":"2025-02-13T16:56:05.212284Z","shell.execute_reply.started":"2025-02-13T16:56:04.947349Z","shell.execute_reply":"2025-02-13T16:56:05.211012Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"We split the id column into id_house and date columns like its in the train","metadata":{}},{"cell_type":"code","source":"sub['date'] = sub['id'].apply(lambda x:x.split('_')[0])\nsub['id_house'] = sub['id'].apply(lambda x:int(x.split('_')[1]))\nsub['date'] = pd.to_datetime(sub['date'])\ntrain['date'] = pd.to_datetime(train['date'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:56:06.338649Z","iopub.execute_input":"2025-02-13T16:56:06.339059Z","iopub.status.idle":"2025-02-13T16:56:06.382667Z","shell.execute_reply.started":"2025-02-13T16:56:06.339030Z","shell.execute_reply":"2025-02-13T16:56:06.381315Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"We check how many months we have to make predictions for.","metadata":{"execution":{"iopub.status.busy":"2025-02-09T11:56:57.497076Z","iopub.execute_input":"2025-02-09T11:56:57.497453Z","iopub.status.idle":"2025-02-09T11:56:57.515125Z","shell.execute_reply.started":"2025-02-09T11:56:57.497412Z","shell.execute_reply":"2025-02-09T11:56:57.513594Z"}}},{"cell_type":"code","source":"sub['date'].unique()\n# 3 month to predict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:50:57.631404Z","iopub.execute_input":"2025-02-13T16:50:57.631724Z","iopub.status.idle":"2025-02-13T16:50:57.641307Z","shell.execute_reply.started":"2025-02-13T16:50:57.631670Z","shell.execute_reply":"2025-02-13T16:50:57.640207Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<DatetimeArray>\n['2022-06-01 00:00:00', '2022-07-01 00:00:00', '2022-08-01 00:00:00']\nLength: 3, dtype: datetime64[ns]"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"We are making a dataset that we will use for training model and predictions for the test. Since we don't have data from the train for every **month and id_house**, we want to fill the missing data. To do this, we are making a dataset where we have records for each month and id_house from the trend. And we fill the missing data, using data from the  **past months** first, then from the **future months** for id_house. This is not the best way to fill in the form, using data from the future. For example, we can simply **throw them out** of training if we can't fill them out correctly. You can check out different ideas that might work better for yourself.","metadata":{}},{"cell_type":"code","source":"date_range = pd.date_range(train['date'].min(), sub['date'].max(), freq = 'MS').tolist()\ncity_list = []\ntime_list = []\nfor city in sub['id_house'].unique():\n    time_list += date_range\n    city_list += [city] * len(date_range)\ndata = pd.DataFrame()\ndata['id_house'] = city_list\ndata['date'] = time_list\ndata = data.merge(train, on = ['id_house', 'date'], how = 'left')\ndata_other_columns = [x for x in data.columns if x not in ['id_house', 'date']]\n#fill issing data\nfor col in tqdm(data_other_columns):\n    data[col] = data.groupby('id_house', group_keys = False)[col].apply(lambda x:x.fillna(method = 'bfill').fillna(method = 'ffill'))\n\ndata['preds'] = -1\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:56:10.247994Z","iopub.execute_input":"2025-02-13T16:56:10.248314Z","iopub.status.idle":"2025-02-13T16:56:29.491157Z","shell.execute_reply.started":"2025-02-13T16:56:10.248288Z","shell.execute_reply":"2025-02-13T16:56:29.490137Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2346aac1a9504ac09b4b6d4e7040ef78"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-13-cde8cac5753d>:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[col] = data.groupby('id_house', group_keys = False)[col].apply(lambda x:x.fillna(method = 'bfill').fillna(method = 'ffill'))\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"        id_house       date  apart_to_room  num_builds_live    mean_price  \\\n0           6123 2020-01-01            0.0              1.0  43500.000000   \n1           6123 2020-02-01            0.0              1.0  43500.000000   \n2           6123 2020-03-01            0.0              1.0  43500.000000   \n3           6123 2020-04-01            0.0              1.0  43500.000000   \n4           6123 2020-05-01            0.0              1.0  43500.000000   \n...          ...        ...            ...              ...           ...   \n171195      5668 2022-04-01            0.0              9.0  22493.675699   \n171196      5668 2022-05-01            0.0              9.0  22866.712054   \n171197      5668 2022-06-01            0.0              9.0  22866.712054   \n171198      5668 2022-07-01            0.0              9.0  22866.712054   \n171199      5668 2022-08-01            0.0              9.0  22866.712054   \n\n        num_builds_series_live  room_three     med_price  room_four  room_one  \\\n0                          1.0    1.000000  42857.142188        0.0  0.000000   \n1                          1.0    1.000000  42857.142188        0.0  0.000000   \n2                          1.0    1.000000  42857.142188        0.0  0.000000   \n3                          1.0    1.000000  42857.142188        0.0  0.000000   \n4                          1.0    1.000000  42857.142188        0.0  0.000000   \n...                        ...         ...           ...        ...       ...   \n171195                     1.0    0.052632  22173.914062        0.0  0.052632   \n171196                     1.0    0.047619  22500.000000        0.0  0.047619   \n171197                     1.0    0.047619  22500.000000        0.0  0.047619   \n171198                     1.0    0.047619  22500.000000        0.0  0.047619   \n171199                     1.0    0.047619  22500.000000        0.0  0.047619   \n\n        ...  room_two  vc_city_quadkey  healthcare_cnt  flats_cnt  beauty_cnt  \\\n0       ...  0.000000              9.0             0.0        1.0         0.0   \n1       ...  0.000000              9.0             0.0        1.0         0.0   \n2       ...  0.000000              9.0             0.0        1.0         0.0   \n3       ...  0.000000              9.0             0.0        1.0         0.0   \n4       ...  0.000000              9.0             0.0        1.0         0.0   \n...     ...       ...              ...             ...        ...         ...   \n171195  ...  0.894737             23.0             3.0        0.0         0.0   \n171196  ...  0.904762             23.0             3.0        0.0         0.0   \n171197  ...  0.904762             23.0             3.0        0.0         0.0   \n171198  ...  0.904762             23.0             3.0        0.0         0.0   \n171199  ...  0.904762             23.0             3.0        0.0         0.0   \n\n        shopping_cnt  build_year_median        lng        lat  preds  \n0                0.0             1981.5  59.107842  79.032814     -1  \n1                0.0             1981.5  59.107842  79.032814     -1  \n2                0.0             1981.5  59.107842  79.032814     -1  \n3                0.0             1981.5  59.107842  79.032814     -1  \n4                0.0             1981.5  59.107842  79.032814     -1  \n...              ...                ...        ...        ...    ...  \n171195           0.0             1972.0  62.612480  77.772386     -1  \n171196           0.0             1972.0  62.612480  77.772386     -1  \n171197           0.0             1972.0  62.612480  77.772386     -1  \n171198           0.0             1972.0  62.612480  77.772386     -1  \n171199           0.0             1972.0  62.612480  77.772386     -1  \n\n[171200 rows x 23 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_house</th>\n      <th>date</th>\n      <th>apart_to_room</th>\n      <th>num_builds_live</th>\n      <th>mean_price</th>\n      <th>num_builds_series_live</th>\n      <th>room_three</th>\n      <th>med_price</th>\n      <th>room_four</th>\n      <th>room_one</th>\n      <th>...</th>\n      <th>room_two</th>\n      <th>vc_city_quadkey</th>\n      <th>healthcare_cnt</th>\n      <th>flats_cnt</th>\n      <th>beauty_cnt</th>\n      <th>shopping_cnt</th>\n      <th>build_year_median</th>\n      <th>lng</th>\n      <th>lat</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6123</td>\n      <td>2020-01-01</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>43500.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>42857.142188</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1981.5</td>\n      <td>59.107842</td>\n      <td>79.032814</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6123</td>\n      <td>2020-02-01</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>43500.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>42857.142188</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1981.5</td>\n      <td>59.107842</td>\n      <td>79.032814</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6123</td>\n      <td>2020-03-01</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>43500.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>42857.142188</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1981.5</td>\n      <td>59.107842</td>\n      <td>79.032814</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6123</td>\n      <td>2020-04-01</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>43500.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>42857.142188</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1981.5</td>\n      <td>59.107842</td>\n      <td>79.032814</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6123</td>\n      <td>2020-05-01</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>43500.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>42857.142188</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1981.5</td>\n      <td>59.107842</td>\n      <td>79.032814</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>171195</th>\n      <td>5668</td>\n      <td>2022-04-01</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>22493.675699</td>\n      <td>1.0</td>\n      <td>0.052632</td>\n      <td>22173.914062</td>\n      <td>0.0</td>\n      <td>0.052632</td>\n      <td>...</td>\n      <td>0.894737</td>\n      <td>23.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1972.0</td>\n      <td>62.612480</td>\n      <td>77.772386</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>171196</th>\n      <td>5668</td>\n      <td>2022-05-01</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>22866.712054</td>\n      <td>1.0</td>\n      <td>0.047619</td>\n      <td>22500.000000</td>\n      <td>0.0</td>\n      <td>0.047619</td>\n      <td>...</td>\n      <td>0.904762</td>\n      <td>23.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1972.0</td>\n      <td>62.612480</td>\n      <td>77.772386</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>171197</th>\n      <td>5668</td>\n      <td>2022-06-01</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>22866.712054</td>\n      <td>1.0</td>\n      <td>0.047619</td>\n      <td>22500.000000</td>\n      <td>0.0</td>\n      <td>0.047619</td>\n      <td>...</td>\n      <td>0.904762</td>\n      <td>23.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1972.0</td>\n      <td>62.612480</td>\n      <td>77.772386</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>171198</th>\n      <td>5668</td>\n      <td>2022-07-01</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>22866.712054</td>\n      <td>1.0</td>\n      <td>0.047619</td>\n      <td>22500.000000</td>\n      <td>0.0</td>\n      <td>0.047619</td>\n      <td>...</td>\n      <td>0.904762</td>\n      <td>23.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1972.0</td>\n      <td>62.612480</td>\n      <td>77.772386</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>171199</th>\n      <td>5668</td>\n      <td>2022-08-01</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>22866.712054</td>\n      <td>1.0</td>\n      <td>0.047619</td>\n      <td>22500.000000</td>\n      <td>0.0</td>\n      <td>0.047619</td>\n      <td>...</td>\n      <td>0.904762</td>\n      <td>23.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1972.0</td>\n      <td>62.612480</td>\n      <td>77.772386</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>171200 rows × 23 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"An example of feature generation using coordinates. We can search for the nearest buildings and average the target values. So we may be making the feature more resistant to outliers.","metadata":{}},{"cell_type":"code","source":"coords_data = data.drop_duplicates('id_house')[['lat', 'lng']]\nnn = NearestNeighbors(n_neighbors = 10) #15\nnn.fit(coords_data.values)\n\ncol = 'mean_price'\ndict_vals = data.groupby(['id_house', 'date'])[col].mean().to_dict()\ndict_ind_house = {i:k for i, k in enumerate(data['id_house'].unique())}\nlist_month = data['date'].unique()\n\ndict_mean = {}\ndict_std = {}\nfor neighbors in nn.kneighbors(coords_data.values)[1]:\n    for month in list_month:\n        vals = []\n        for neighbor in neighbors:\n            if (dict_ind_house[neighbor], month) in dict_vals:\n                vals += [dict_vals[(dict_ind_house[neighbor], month)]]\n        if len(vals) > 0:\n            dict_mean[(dict_ind_house[neighbors[0]], month)] = np.mean(vals)\n\ndata['mean_pricem'] = [dict_mean.get((c, m), None) for c,m in data[['id_house', 'date']].values]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:56:29.492294Z","iopub.execute_input":"2025-02-13T16:56:29.492601Z","iopub.status.idle":"2025-02-13T16:56:34.056855Z","shell.execute_reply.started":"2025-02-13T16:56:29.492571Z","shell.execute_reply":"2025-02-13T16:56:34.055317Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"Functions for training catboost and lightgbm","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_percentage_error\nimport xgboost as xgb\ndef catboost_train(train, target, split_list, param):\n    \n    bst_list = []\n    for i , (train_index, val_index, test_index) in enumerate(split_list):\n\n        tr = catboost.Pool(train[train_index], label = target[train_index])\n        te = catboost.Pool(train[val_index], label = target[val_index])\n        \n        bst = catboost.train(tr, param, eval_set = te, iterations = 2000, early_stopping_rounds = 100, verbose =300)\n        bst_list += [bst]\n\n        gc.collect()\n        del tr, te\n    \n    return bst_list\n\nparams_cat = {\n    'loss_function' :'MAE', \n     'max_depth' : 4, \n    'eval_metric' :'MAPE', \n    'learning_rate' : .01, \n    'l2_leaf_reg' : 15, \n    'random_state' : 42 ,\n    'random_strength' : 1,\n    'grow_policy' : 'Depthwise',\n    'bagging_temperature' : 2,\n    #'subsample' : 0.85,\n    'bootstrap_type' :  'Bayesian',\n}\n\ndef lgb_train(train, target, split_list, param):\n    \n    bst_list = []\n    for i , (train_index, val_index, test_index) in enumerate(split_list):\n\n        tr = lgb.Dataset(train[train_index], target[train_index])\n        te = lgb.Dataset(train[val_index], target[val_index], reference=tr)\n    \n        bst = lgb.train(param, tr, num_boost_round = 2000, valid_sets = te,\n                        callbacks = [lgb.early_stopping(100), lgb.log_evaluation(5000)])\n        bst_list += [bst]\n\n        gc.collect()\n        del tr, te\n    \n    return bst_list\n\nparams_lgb = {\n    'objective':        'mae',\n    'verbosity':        -1,\n    'boosting_type':    'gbdt',\n    'metric' : 'mape',\n    'lambda_l1':        7,\n    'lambda_l2':        5,\n    'learning_rate':    0.01,\n    'num_leaves':        16,\n    'extra_trees' : True,\n}\ndef xgb_train(train, target, split_list, param):\n    \n    bst_list = []\n    \n    for i, (train_index, val_index, test_index) in enumerate(split_list):\n        \n        tr = xgb.DMatrix(train[train_index], label=target[train_index])\n        te = xgb.DMatrix(train[val_index], label=target[val_index])\n        \n        evallist = [(te, 'eval')]\n        \n        bst = xgb.train(param, tr, num_boost_round=2000,\n                        evals=evallist,\n                        early_stopping_rounds=100,\n                        verbose_eval=5000)\n        \n        bst_list += [bst]\n        \n        gc.collect()\n        del tr, te\n    \n    return bst_list\n\nparams_xgb = {\n    'objective': 'reg:absoluteerror',  \n    'learning_rate': 0.01,\n    'max_depth': 4,                 \n    'min_child_weight': 3,           \n    'reg_alpha': 5,              \n    'reg_lambda': 5,         \n    'gamma': 1,\n    'random_state': 42\n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:56:34.058605Z","iopub.execute_input":"2025-02-13T16:56:34.058993Z","iopub.status.idle":"2025-02-13T16:56:34.067642Z","shell.execute_reply.started":"2025-02-13T16:56:34.058973Z","shell.execute_reply":"2025-02-13T16:56:34.066550Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Since we need to make predictions for 3 months ahead. We will build models separately for each month. And then we'll combine the predictions. As features, we will use the values of the various columns for which we apply **rolling mean and shift** with number = the month for which we are making a prediction. Plus i use year as a feature\n\nFor validation i using 2 last month in train. Then train catboost and lightgbm and average predictions from that models. Final prediction i save in columns **preds** and than I merge that column with the sample_submission.csv file. Also i check correlations.","metadata":{}},{"cell_type":"code","source":"train_month = sorted(train['date'].unique())\ntest_month = sorted(sub['date'].unique())\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nfor m_predict in [1, 2, 3]:\n\n    data[f'mean_price_shift_{m_predict}'] = data.groupby('id_house', group_keys = False)['mean_price'].apply(lambda x:x.shift(m_predict))\n    data['new_target'] = data['mean_price'] / data[f'mean_price_shift_{m_predict}']\n\n    data['year'] = data['date'].dt.year\n    \n    train_cols = ['year']\n    \n    for col in tqdm(['build_year_median', 'vc_city_quadkey', 'number_total', 'new_target', 'mean_price',\n                    'num_builds_live', 'room_four', 'mean_pricem','room_one']):\n        data[f'{col}_shift_rm_{m_predict}'] = data.groupby('id_house', group_keys = False)[col].apply(lambda x:\n                                            x.rolling(3, min_periods = 1).mean().shift(m_predict))\n        train_cols += [f'{col}_shift_rm_{m_predict}']\n    \n    '''train_cols2 = ['build_year_median', 'vc_city_quadkey', f'new_target_shift_rm_{m_predict}']\n    for col in tqdm(['number_total','mean_price','mean_pricem', 'room_four','num_builds_live']):\n            data[f'{col}_diff_{m_predict}'] = data.groupby('id_house', group_keys = False)[col].apply(lambda x:\n                                                 x.shift(m_predict) - x)\n            train_cols2 += [f'{col}_diff_{m_predict}']'''\n\n    split_list = []    \n    list_val_months = [-1, -2]\n    for val_month in list_val_months:\n        train_index = data[(data['date'] > train_month[5]) & (data['date'] <= train_month[val_month - 1])].index\n        val_index = data[data['date'] == train_month[val_month]].index\n        test_index = data[data['date'] == test_month[m_predict - 1]].index\n        split_list += [(train_index, val_index, test_index)]\n\n    # CHECKING CORRELATIONS\n    vals = data[train_cols].corr().abs().values\n    print('CHECK CORR COLS', bool(vals[~np.eye(vals.shape[0],dtype=bool)].max() < 0.95) == True, \n          'MAX CORR: ', vals[~np.eye(vals.shape[0],dtype=bool)].max() )\n\n    \n    bst_list_catboost = catboost_train(data[train_cols].values, data['new_target'].values, split_list, params_cat)\n\n    catboost_preds = []\n    for num_, bst in enumerate(bst_list_catboost):\n        val_index = split_list[num_][-2]\n        val_pred = bst.predict(data[train_cols].values[val_index]) * data[f'mean_price_shift_{m_predict}'][val_index]\n        score = (data['mean_price'][val_index] - val_pred).abs().mean()\n        print(f'VAL SCORE CATBOOST MONTH {num_}: ', score)\n        test_index = split_list[num_][-1]\n        catboost_preds += [ bst.predict(data[train_cols].values[test_index]) * data[f'mean_price_shift_{m_predict}'][test_index] ]\n    catboost_preds = np.mean(catboost_preds, 0)\n\n    bst_list_lgb = lgb_train(data[train_cols].values, data['new_target'].values, split_list, params_lgb)\n    lgb_preds = []\n    for num_, bst in enumerate(bst_list_lgb):\n        val_index = split_list[num_][-2]\n        val_pred = bst.predict(data[train_cols].values[val_index]) * data[f'mean_price_shift_{m_predict}'][val_index]\n        score = (data['mean_price'][val_index] - val_pred).abs().mean()\n        print(f'VAL SCORE LIGHTGBM MONTH {num_}: ', score)\n        test_index = split_list[num_][-1]\n        lgb_preds += [ bst.predict(data[train_cols].values[test_index]) * data[f'mean_price_shift_{m_predict}'][test_index] ]\n    lgb_preds = np.mean(lgb_preds, 0)\n\n    bst_list_xgb = xgb_train(data[train_cols].values, data['new_target'].values, split_list, params_xgb)\n    xgb_preds = []\n    for num_, bst in enumerate(bst_list_xgb):\n        val_index = split_list[num_][-2]\n        val_data = xgb.DMatrix(data[train_cols].values[val_index])\n        val_pred = bst.predict(val_data) * data[f'mean_price_shift_{m_predict}'][val_index]\n        score = (data['mean_price'][val_index] - val_pred).abs().mean()\n        print(f'VAL SCORE XGB MONTH {num_}: ', score)\n        test_index = split_list[num_][-1]\n        xgb_preds += [ bst.predict(xgb.DMatrix(data[train_cols].values[test_index])) * data[f'mean_price_shift_{m_predict}'][test_index] ]\n    xgb_preds = np.mean(xgb_preds, 0)\n\n    \n    data.loc[test_index, 'preds'] = (catboost_preds + lgb_preds + xgb_preds) / 3\n    # hui\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:57:36.661001Z","iopub.execute_input":"2025-02-13T16:57:36.661314Z","iopub.status.idle":"2025-02-13T17:01:31.952927Z","shell.execute_reply.started":"2025-02-13T16:57:36.661288Z","shell.execute_reply":"2025-02-13T17:01:31.952345Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c70fc3cf774844ad8cfc04d06acd07"}},"metadata":{}},{"name":"stdout","text":"CHECK CORR COLS True MAX CORR:  0.9292495275428049\n0:\tlearn: 0.0170505\ttest: 0.0161136\tbest: 0.0161136 (0)\ttotal: 20.5ms\tremaining: 40.9s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.01598849208\nbestIteration = 195\n\nShrink model to first 196 iterations.\n0:\tlearn: 0.0170298\ttest: 0.0174846\tbest: 0.0174846 (0)\ttotal: 18.5ms\tremaining: 37s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.01735354303\nbestIteration = 185\n\nShrink model to first 186 iterations.\nVAL SCORE CATBOOST MONTH 0:  2229.4735030085403\nVAL SCORE CATBOOST MONTH 1:  2397.1583558143334\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[329]\tvalid_0's mape: 0.0159959\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[569]\tvalid_0's mape: 0.0173576\nVAL SCORE LIGHTGBM MONTH 0:  2231.0373404716183\nVAL SCORE LIGHTGBM MONTH 1:  2399.117893551519\n[0]\teval-mae:0.01706\n[283]\teval-mae:0.01693\n[0]\teval-mae:0.01865\n[271]\teval-mae:0.01852\nVAL SCORE XGB MONTH 0:  2229.833867803184\nVAL SCORE XGB MONTH 1:  2398.03410699625\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"875a03e7944842f8bc2f0d8e6955e373"}},"metadata":{}},{"name":"stdout","text":"CHECK CORR COLS True MAX CORR:  0.9289505307628535\n0:\tlearn: 0.0272533\ttest: 0.0268541\tbest: 0.0268541 (0)\ttotal: 18.8ms\tremaining: 37.5s\n300:\tlearn: 0.0267081\ttest: 0.0263725\tbest: 0.0263649 (228)\ttotal: 5.3s\tremaining: 29.9s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.02636487287\nbestIteration = 228\n\nShrink model to first 229 iterations.\n0:\tlearn: 0.0271761\ttest: 0.0288663\tbest: 0.0288663 (0)\ttotal: 22.4ms\tremaining: 44.7s\n300:\tlearn: 0.0266354\ttest: 0.0281918\tbest: 0.0281918 (300)\ttotal: 5.08s\tremaining: 28.7s\n600:\tlearn: 0.0266014\ttest: 0.0281692\tbest: 0.0281692 (600)\ttotal: 10s\tremaining: 23.4s\n900:\tlearn: 0.0265808\ttest: 0.0281551\tbest: 0.0281550 (898)\ttotal: 15.1s\tremaining: 18.4s\n1200:\tlearn: 0.0265645\ttest: 0.0281484\tbest: 0.0281483 (1182)\ttotal: 19.9s\tremaining: 13.2s\n1500:\tlearn: 0.0265466\ttest: 0.0281425\tbest: 0.0281418 (1488)\ttotal: 24.7s\tremaining: 8.21s\n1800:\tlearn: 0.0265307\ttest: 0.0281365\tbest: 0.0281364 (1792)\ttotal: 29.5s\tremaining: 3.26s\n1999:\tlearn: 0.0265212\ttest: 0.0281327\tbest: 0.0281327 (1998)\ttotal: 32.7s\tremaining: 0us\n\nbestTest = 0.02813266601\nbestIteration = 1998\n\nShrink model to first 1999 iterations.\nVAL SCORE CATBOOST MONTH 0:  3594.7540752089385\nVAL SCORE CATBOOST MONTH 1:  3856.986759572935\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[1998]\tvalid_0's mape: 0.026351\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[1999]\tvalid_0's mape: 0.0281644\nVAL SCORE LIGHTGBM MONTH 0:  3592.8512978321146\nVAL SCORE LIGHTGBM MONTH 1:  3868.388538942812\n[0]\teval-mae:0.02907\n[294]\teval-mae:0.02855\n[0]\teval-mae:0.03141\n[1089]\teval-mae:0.03062\nVAL SCORE XGB MONTH 0:  3601.47482386797\nVAL SCORE XGB MONTH 1:  3851.687826650672\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a4a898cff2d46c5be39f3f79da3db0b"}},"metadata":{}},{"name":"stdout","text":"CHECK CORR COLS True MAX CORR:  0.928641019696598\n0:\tlearn: 0.0340166\ttest: 0.0353450\tbest: 0.0353450 (0)\ttotal: 19.4ms\tremaining: 38.9s\n300:\tlearn: 0.0330020\ttest: 0.0341189\tbest: 0.0341178 (284)\ttotal: 5.12s\tremaining: 28.9s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.03411141666\nbestIteration = 431\n\nShrink model to first 432 iterations.\n0:\tlearn: 0.0338774\ttest: 0.0369550\tbest: 0.0369550 (0)\ttotal: 17.9ms\tremaining: 35.8s\n300:\tlearn: 0.0329054\ttest: 0.0352646\tbest: 0.0352554 (293)\ttotal: 4.97s\tremaining: 28s\n600:\tlearn: 0.0328524\ttest: 0.0352074\tbest: 0.0352074 (600)\ttotal: 9.78s\tremaining: 22.8s\n900:\tlearn: 0.0328037\ttest: 0.0351537\tbest: 0.0351537 (899)\ttotal: 14.8s\tremaining: 18s\n1200:\tlearn: 0.0327673\ttest: 0.0351211\tbest: 0.0351210 (1199)\ttotal: 19.5s\tremaining: 12.9s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.03511191939\nbestIteration = 1350\n\nShrink model to first 1351 iterations.\nVAL SCORE CATBOOST MONTH 0:  4646.426393849468\nVAL SCORE CATBOOST MONTH 1:  4871.758640469549\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[1998]\tvalid_0's mape: 0.0340287\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[1999]\tvalid_0's mape: 0.0352731\nVAL SCORE LIGHTGBM MONTH 0:  4641.097703114094\nVAL SCORE LIGHTGBM MONTH 1:  4890.90174311092\n[0]\teval-mae:0.03882\n[800]\teval-mae:0.03739\n[0]\teval-mae:0.04047\n[1533]\teval-mae:0.03844\nVAL SCORE XGB MONTH 0:  4633.487017849299\nVAL SCORE XGB MONTH 1:  4846.420651268542\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"making submission","metadata":{}},{"cell_type":"code","source":"sub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:34:07.819493Z","iopub.execute_input":"2025-02-11T11:34:07.820784Z","iopub.status.idle":"2025-02-11T11:34:07.837728Z","shell.execute_reply.started":"2025-02-11T11:34:07.820746Z","shell.execute_reply":"2025-02-11T11:34:07.836238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub = sub.merge(data[['date', 'id_house', 'preds']], on = ['date', 'id_house'], how = 'left')\nsub['target'] = sub['preds']\n# delete temp files of catboost from working folder\n!rm -rf /kaggle/working/\nsub[['id', 'target']].to_csv('solution.csv', index = None)\nsub['target'].min(), sub['target'].isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:01:31.954178Z","iopub.execute_input":"2025-02-13T17:01:31.954437Z","iopub.status.idle":"2025-02-13T17:01:32.148442Z","shell.execute_reply.started":"2025-02-13T17:01:31.954412Z","shell.execute_reply":"2025-02-13T17:01:32.147064Z"}},"outputs":[{"name":"stdout","text":"rm: cannot remove '/kaggle/working/': Device or resource busy\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(12399.999636020199, 0)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import os\nos.makedirs(\"/root/.kaggle\", exist_ok=True)\ntext_file = open(\"/root/.kaggle/kaggle.json\", \"w\")\nn = text_file.write('{\"username\":\"heriqis777\",\"key\":\"51f5d196d31e37053d573e0679743000\"}')\ntext_file.close()\n!chmod 600 /root/.kaggle/kaggle.json\n# Make sure it worked\n!cat /root/.kaggle/kaggle.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:01:32.150111Z","iopub.execute_input":"2025-02-13T17:01:32.150388Z","iopub.status.idle":"2025-02-13T17:01:32.443494Z","shell.execute_reply.started":"2025-02-13T17:01:32.150365Z","shell.execute_reply":"2025-02-13T17:01:32.441898Z"}},"outputs":[{"name":"stdout","text":"{\"username\":\"heriqis777\",\"key\":\"51f5d196d31e37053d573e0679743000\"}","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!kaggle competitions submit -c ioai-2025-preparation-class-lesson-4-homework -f 'solution.csv' -m \"{final}\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:01:32.445085Z","iopub.execute_input":"2025-02-13T17:01:32.445469Z","iopub.status.idle":"2025-02-13T17:01:36.790000Z","shell.execute_reply.started":"2025-02-13T17:01:32.445436Z","shell.execute_reply":"2025-02-13T17:01:36.788359Z"}},"outputs":[{"name":"stdout","text":"100%|█████████████████████████████████████████| 521k/521k [00:01<00:00, 356kB/s]\nSuccessfully submitted to IOAI 2025 preparation class, Lesson 4, Homework","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"набирает 3786","metadata":{}},{"cell_type":"markdown","source":"Function for checking correlation between features. Its should output: True","metadata":{}},{"cell_type":"code","source":"train_features = ['apart_to_room', 'num_builds_live', 'med_price', 'room_four']\nvals = train[train_features].corr().abs().values\nprint(vals)\nbool(vals[~np.eye(vals.shape[0],dtype=bool)].max() < 0.95) == True","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}